{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from string import hexdigits\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '/home/welling/Downloads/Split upload uuid tracking.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_uuid(s):\n",
    "    return(len(s) == 32 and all(c in hexdigits for c in s))\n",
    "def is_hubmap_id(s):\n",
    "    return(len(s) == 15 and s.startswith('HBM'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser for a .txt file downloaded from the \"Split upload uuid tracking\" doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rec_generator(fname):\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            yield line\n",
    "\n",
    "class Tokenizer():\n",
    "    def __init__(self, fname):\n",
    "        self.tok_l = []\n",
    "        self.rec_num = 0\n",
    "        self.rec_gen = create_rec_generator(fname)\n",
    "    def next_line(self):\n",
    "        self.tok_l += reversed(self.rec_gen.__next__().split())\n",
    "        self.rec_num += 1\n",
    "    def get(self):\n",
    "        try:\n",
    "            while not self.tok_l:\n",
    "                self.next_line()\n",
    "            rslt = self.tok_l.pop()\n",
    "            #print(f'get: {self.tok_l} <{rslt}>')\n",
    "            return rslt\n",
    "        except StopIteration:\n",
    "            return None\n",
    "    def pushback(self,tok):\n",
    "        if tok != '':\n",
    "            self.tok_l.append(tok)\n",
    "            #print(f'pushback <{tok}> -> {self.tok_l}')\n",
    "\n",
    "# tokenizer test\n",
    "#tzr = Tokenizer(multi_file)\n",
    "#while True:\n",
    "#    tok = tzr.get()\n",
    "#    if tok is None:\n",
    "#        break\n",
    "\n",
    "hits = []\n",
    "state = 'empty'\n",
    "currently_being_decomposed = None\n",
    "children_of_this_decomposition = []\n",
    "found_groups = []\n",
    "tzr = Tokenizer(fname)\n",
    "dbg = False\n",
    "dbg_ct = 100\n",
    "tok = None\n",
    "while True:\n",
    "    prev_tok = tok\n",
    "    tok = tzr.get()\n",
    "    #if tok and \"bbdda69f7d867e20486f66ac2ebbd5b4\" in tok:\n",
    "    #   dbg = True\n",
    "    if dbg:\n",
    "        if dbg_ct > 0:\n",
    "            print(f'top: state=<{state}> tok=<{tok}> currently_being_decomposed=<{currently_being_decomposed}>')\n",
    "            dbg_ct -= 1\n",
    "        else:\n",
    "            dbg = False\n",
    "    if tok is None:\n",
    "        break\n",
    "    elif tok == '':\n",
    "        #print('skip space')\n",
    "        pass\n",
    "    elif state in [\"empty\"]:\n",
    "        if tok.lower() in [\"decomposition\", \"decomposing\"]:\n",
    "            if children_of_this_decomposition is not None:\n",
    "                this_group = {\n",
    "                    \"parent\": currently_being_decomposed,\n",
    "                    \"children\": children_of_this_decomposition\n",
    "                }\n",
    "                children_of_this_decomposition = []\n",
    "                pprint(this_group)\n",
    "                found_groups.append(this_group)\n",
    "            if tok.lower() in [\"decomposition\"]:\n",
    "                state = \"decomp\"\n",
    "            elif tok.lower() in [\"decomposing\"]:\n",
    "                state = \"of\"\n",
    "            else:\n",
    "                print(f'exiting with state {state} tok {tok} prev_tok {prev_tok}')\n",
    "                raise RuntimeError(\"Bad parse\")\n",
    "        else:\n",
    "            pass\n",
    "    elif state == \"decomp\":\n",
    "        if tok.lower() in [\"of\"]:\n",
    "            state = \"of\"\n",
    "        else:\n",
    "            pass\n",
    "    elif state == \"of\":\n",
    "        if is_uuid(tok):\n",
    "            currently_being_decomposed = tok\n",
    "            print(f'currently_being_decomposed <- {currently_being_decomposed}')\n",
    "            state = \"inner\"\n",
    "        else:\n",
    "            pass\n",
    "    elif state == \"inner\":\n",
    "        if tok in [\"->\"]:\n",
    "            state = \"arrow\"\n",
    "        elif tok.lower() in [\"decomposition\", \"decomposing\"]:\n",
    "            this_group = {\n",
    "                \"parent\": currently_being_decomposed,\n",
    "                \"children\": children_of_this_decomposition\n",
    "            }\n",
    "            children_of_this_decomposition = []\n",
    "            pprint(this_group)\n",
    "            found_groups.append(this_group)\n",
    "            if tok.lower() in [\"decomposition\"]:\n",
    "                state = \"decomp\"\n",
    "            elif tok.lower() in [\"decomposing\"]:\n",
    "                state = \"of\"\n",
    "            else:\n",
    "                print(f'exiting with state {state} tok {tok} prev_tok {prev_tok}')\n",
    "                raise RuntimeError(\"Bad parse\")\n",
    "        else:\n",
    "            pass\n",
    "    elif state == \"arrow\":\n",
    "        if is_uuid(tok):\n",
    "            children_of_this_decomposition.append(tok)\n",
    "        state = \"inner\"\n",
    "\n",
    "if state == \"inner\" and currently_being_decomposed is not None:\n",
    "    this_group = {\n",
    "        \"parent\": currently_being_decomposed,\n",
    "        \"children\": children_of_this_decomposition\n",
    "    }\n",
    "    pprint(this_group)\n",
    "    found_groups.append(this_group)\n",
    "\n",
    "        \n",
    "#    if tzr.rec_num > 100:\n",
    "#        break\n",
    "\n",
    "#pprint(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = []\n",
    "for group in found_groups:\n",
    "    parent = group['parent']\n",
    "    for elt in group['children']:\n",
    "        recs.append({'uuid':elt, 'parent':parent})\n",
    "df = pd.DataFrame(recs)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/tmp/all_products_of_split_uploads.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (hubmapEnv)",
   "language": "python",
   "name": "hubmapenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
