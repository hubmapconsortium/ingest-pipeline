{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from string import hexdigits\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs:\n",
    "* big_sheet is the standard spreadsheet of unpublished datasets generated by new_dataset_survey.py .\n",
    "* multi_file is a text file exported from the shared doc \"Multi-dataset run JSON strings\"\n",
    "* published_dataset_uuid_list is a csv file containing the column 'uuid', with the uuids of all existing published datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_sheet = \"/tmp/unpublished_datasets.tsv\"\n",
    "\n",
    "multi_file = '/home/welling/Downloads/Multi-dataset run JSON strings.txt'\n",
    "\n",
    "published_dataset_uuid_list = \"/home/welling/Downloads/unique_published_dataset_uuids.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_uuid(s):\n",
    "    return(len(s) == 32 and all(c in hexdigits for c in s))\n",
    "def is_hubmap_id(s):\n",
    "    return(len(s) == 15 and s.startswith('HBM'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the big table of unpublished datasets\n",
    "\n",
    "The interesting products of this are:\n",
    "* **qa_df_short** : all QA datasets\n",
    "* **qa_primary** : all QA primary datasets\n",
    "* **qa_derived** : all QA derived datasets\n",
    "* **matched_groups_df** : each row contains a primary dataset and its single derived child\n",
    "* **qa_datasets_set** : a set containing the uuids of all datasets in QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df = pd.read_csv(big_sheet, sep='\\t')\n",
    "\n",
    "qa_df = big_df[big_df.status == 'QA']\n",
    "\n",
    "qa_df_short = qa_df.drop(columns=['donor_hubmap_id', 'donor_submission_id', 'donor_uuid', 'organ', 'provider_experiment_id',\n",
    "                         'sample_hubmap_id', 'sample_submission_id', 'status', 'has_data', 'has_metadata',\n",
    "                         'n_md_recs', 'validated', 'last_touch'])\n",
    "\n",
    "qa_primary = qa_df_short[qa_df_short.is_derived==False]\n",
    "\n",
    "qa_derived = qa_df_short[qa_df_short.is_derived==True]\n",
    "\n",
    "qa_datasets_set = set(qa_df_short['uuid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thing = pd.merge(how='left', left_on='uuid', right_on='parent_dataset', left=qa_primary, right=qa_derived)\n",
    "\n",
    "thing=thing.dropna(subset=['uuid_y'])\n",
    "\n",
    "mismatched_groups = []\n",
    "matched_groups = []\n",
    "for idx, row in thing.iterrows():\n",
    "    if row['group_name_x'] != row['group_name_y']:\n",
    "        mismatched_groups.append(row)\n",
    "    else:\n",
    "        matched_groups.append(row)\n",
    "mismatched_groups_df = pd.DataFrame(mismatched_groups)\n",
    "assert len(mismatched_groups_df) == 0\n",
    "matched_groups_df = pd.DataFrame(matched_groups)\n",
    "\n",
    "matched_groups_df = matched_groups_df.drop(columns=['is_derived_x', 'is_derived_y', 'parent_dataset_x',\n",
    "                                                       'group_name_y', 'is_derived_y', 'parent_dataset_y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we need to manually check that each of the left-hand uuids (the primary datasets) has only 1 derived dataset.  If there are 2, it would imply that a run meant to be discarded was not properly cleaned up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = matched_groups_df.groupby('uuid_x')\n",
    "\n",
    "print(\"Primaries with more than one derived:\")\n",
    "for key, group in groups:\n",
    "    if len(group) != 1:\n",
    "        print(key,len(group))\n",
    "print(\"(end of list)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually inspect the notes for the matched pairs for signs that the derived dataset should not be published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in matched_groups_df.iterrows():\n",
    "    if not pd.isna(row['note_x']):\n",
    "        print(f\"{row['uuid_x']}: {row['note_x']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the csv of Published dataset uuids\n",
    "The product here is published_datasets_set, a set containing all Published uuids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_datasets_df = pd.read_csv(published_dataset_uuid_list)\n",
    "\n",
    "published_datasets_set = set(published_datasets_df['uuid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The next block contains a parser for the Multiple Dataset Runs table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rec_generator(fname):\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            yield line\n",
    "\n",
    "class Tokenizer():\n",
    "    def __init__(self, fname):\n",
    "        self.tok_l = []\n",
    "        self.rec_num = 0\n",
    "        self.rec_gen = create_rec_generator(fname)\n",
    "    def next_line(self):\n",
    "        self.tok_l += reversed(self.rec_gen.__next__().split())\n",
    "        self.rec_num += 1\n",
    "    def get(self):\n",
    "        try:\n",
    "            while not self.tok_l:\n",
    "                self.next_line()\n",
    "            rslt = self.tok_l.pop()\n",
    "            #print(f'get: {self.tok_l} <{rslt}>')\n",
    "            return rslt\n",
    "        except StopIteration:\n",
    "            return None\n",
    "    def pushback(self,tok):\n",
    "        if tok != '':\n",
    "            self.tok_l.append(tok)\n",
    "            #print(f'pushback <{tok}> -> {self.tok_l}')\n",
    "\n",
    "# tokenizer test\n",
    "#tzr = Tokenizer(multi_file)\n",
    "#while True:\n",
    "#    tok = tzr.get()\n",
    "#    if tok is None:\n",
    "#        break\n",
    "\n",
    "hits = []\n",
    "state = 'empty'\n",
    "current_json = \"\"\n",
    "tzr = Tokenizer(multi_file)\n",
    "while True:\n",
    "    tok = tzr.get()\n",
    "    #print(f'top: state=<{state} tok=<{tok}>')\n",
    "    if tok is None:\n",
    "        break\n",
    "    elif tok == '':\n",
    "        #print('skip space')\n",
    "        pass\n",
    "    elif state == \"empty\":\n",
    "        if '{' in tok:\n",
    "            parts = tok.split('{', 2)\n",
    "            current_json = \"{\"\n",
    "            state = \"in_json\"\n",
    "            #print(f'point 1: {parts}')\n",
    "            for word in parts:\n",
    "                tzr.pushback(word)\n",
    "        else:\n",
    "            pass\n",
    "    elif state == 'in_json':\n",
    "        if '}' in tok:\n",
    "            parts = tok.split('}', 2)\n",
    "            current_json = current_json + ' ' + parts[0] + \" }\"\n",
    "            state = 'after_json'\n",
    "            #print(f'point 2: {current_json}')\n",
    "            tzr.pushback(parts[1])\n",
    "        else:\n",
    "            current_json = f\"{current_json} {tok}\"\n",
    "            state = 'in_json'\n",
    "    elif state == 'after_json':\n",
    "        #print(f'point 4a: <{tok}>')\n",
    "        if '-' in tok and '>' in tok:\n",
    "            state = 'after_arrow'\n",
    "            #print(f'point 4b {tok}')\n",
    "        else:\n",
    "            #print(f'point 3 {current_json}')\n",
    "            hits.append((current_json, None))\n",
    "            state = 'empty'\n",
    "    elif state == 'after_arrow':\n",
    "        #print(f'point 5a: {tok}')\n",
    "        if is_uuid(tok):\n",
    "            hits.append((current_json, tok))\n",
    "            state = 'empty'\n",
    "        elif '{' in tok:\n",
    "            hits.append((current_json, None))\n",
    "            parts = tok.split('{', 2)\n",
    "            current_json = \"{\"\n",
    "            state = \"in_json\"\n",
    "            for word in parts:\n",
    "                tzr.pushback(word)\n",
    "        else:\n",
    "            # dangling ->\n",
    "            hits.append((current_json, None))\n",
    "            state = 'empty'\n",
    "              \n",
    "#    if tzr.rec_num > 100:\n",
    "#        break\n",
    "\n",
    "#pprint(hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the multi-dataset run records into categories\n",
    "The uuids used in the multi-dataset runs are sorted into the following groups:\n",
    "* input and output uuids that have already been published\n",
    "* input uuids that are in QA and appear ready to publish\n",
    "* output uuids that are in QA and appear ready to publish\n",
    "* input uuids that are actualy HuBMAP id strings and thus cannot be handles by this script\n",
    "* anomalies, which contains everything else, including un-parsable JSON.\n",
    "\n",
    "The list of anomalies must be examined manually. Each is a tuple containing the uuid, \"output\" if the uuid was a run output, and the ordinal number of the multi-dataset run record from which it came."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_published_l = []\n",
    "ready_l = []\n",
    "ready_outputs_l = []\n",
    "input_hubmap_ids = []\n",
    "anomalies = []\n",
    "elt_counter = 0\n",
    "for json_s, output_uuid in hits:\n",
    "    #print(f\"{json_s} {output_uuid}\")\n",
    "    try:\n",
    "        dct = json.loads(json_s)\n",
    "        for uuid in dct['uuid_list']:\n",
    "            if uuid in published_datasets_set:\n",
    "                already_published_l.append(uuid)\n",
    "            elif uuid in qa_datasets_set:\n",
    "                ready_l.append(uuid)\n",
    "            elif is_hubmap_id(uuid):\n",
    "                input_hubmap_ids.append((uuid, elt_counter))\n",
    "            else:\n",
    "                anomalies.append((uuid, elt_counter))\n",
    "    except json.JSONDecodeError:\n",
    "        anomalies.append((json_s, elt_counter))\n",
    "    if output_uuid is None:\n",
    "        pass\n",
    "    elif output_uuid in published_datasets_set:\n",
    "        already_published_l.append(output_uuid)\n",
    "    elif output_uuid in qa_datasets_set:\n",
    "        ready_outputs_l.append(output_uuid)\n",
    "    else:\n",
    "        anomalies.append((output_uuid, \"output\", elt_counter))\n",
    "    elt_counter += 1\n",
    "print(f\"input_hubmap_ids: {input_hubmap_ids}\")\n",
    "print('ANOMALIES:')\n",
    "for elt in anomalies:\n",
    "    print(elt)\n",
    "print('end anomalies')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A utility function to assemble the record which will ultimately appear in the tsv file of publishable datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uuid_record_dict(uuid, note=None):\n",
    "    one_row_df = qa_df_short[qa_df_short.uuid == uuid]\n",
    "    assert len(one_row_df) == 1\n",
    "    row = one_row_df.iloc[0]\n",
    "    return {\n",
    "        'group_name': row['group_name'],\n",
    "        'data_types': row['data_types'],\n",
    "        'uuid': row['uuid'],\n",
    "        'hubmap_id': row['hubmap_id'],\n",
    "        'note': '' if note is None else note\n",
    "    }\n",
    "#dct = get_uuid_record_dict(ready_l[0], note='foo')\n",
    "#pprint(dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble the final table\n",
    "This block goes through all the lists and tables of uuids accumulated so far and attempts to produce a DataFrame of publishable datasets.  The notes associated with each row tell which block of code produced each entry and can help unravel mysteries.  \n",
    "\n",
    "*Straggler entries must be examined by hand.  Most represent some kind of record-keeping error in the input files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handled_set = set()\n",
    "all_recs = []\n",
    "\n",
    "# This loop covers the inputs to multi runs\n",
    "for uuid in set(ready_l):\n",
    "    assert uuid not in handled_set\n",
    "    all_recs.append(get_uuid_record_dict(uuid, note=\"no unique child\"))\n",
    "    handled_set.add(uuid)\n",
    "\n",
    "# This loop covers the outputs from multi runs\n",
    "for uuid in set(ready_outputs_l):\n",
    "    assert uuid not in handled_set\n",
    "    all_recs.append(get_uuid_record_dict(uuid, note=\"no unique parent\"))\n",
    "    handled_set.add(uuid)\n",
    "\n",
    "# This loop covers all matched pairs\n",
    "for idx, row in matched_groups_df.iterrows():\n",
    "    uuid_primary = row['uuid_x']\n",
    "    hubmapid_primary = row['hubmap_id_x']\n",
    "    group = row['group_name_x']\n",
    "    data_types_primary = row['data_types_x']\n",
    "    uuid_derived = row['uuid_y']\n",
    "    hubmapid_derived = row['hubmap_id_y']\n",
    "    data_types_derived = row['data_types_y']\n",
    "    if (uuid_primary not in published_datasets_set\n",
    "        and uuid_primary not in handled_set):\n",
    "        all_recs.append(get_uuid_record_dict(uuid_primary,\n",
    "                                            note=f\"derived dataset {uuid_derived}\"))\n",
    "        handled_set.add(uuid_primary)\n",
    "    if (uuid_derived not in published_datasets_set\n",
    "       and uuid_derived not in handled_set):\n",
    "        all_recs.append(get_uuid_record_dict(uuid_derived,\n",
    "                                            note=f\"parent dataset {uuid_primary}\"))\n",
    "        handled_set.add(uuid_derived)\n",
    "\n",
    "# This loop covers derived children of published datasets- that is, new versions\n",
    "for idx, row in qa_derived.iterrows():\n",
    "    uuid = row['uuid']\n",
    "    parent_uuid = row['parent_dataset']\n",
    "    if uuid not in handled_set:\n",
    "        if parent_uuid in published_datasets_set:\n",
    "            all_recs.append(get_uuid_record_dict(uuid,\n",
    "                                                note=f\"child of published parent {parent_uuid}\"))\n",
    "            handled_set.add(uuid)\n",
    "\n",
    "# This loop checks for stragglers\n",
    "for idx, row in qa_df_short.iterrows():\n",
    "    uuid = row['uuid']\n",
    "    if uuid not in handled_set:\n",
    "        all_recs.append(get_uuid_record_dict(uuid,\n",
    "                                            note=\"straggler; \" + row['note']))\n",
    "        handled_set.add(uuid)\n",
    "\n",
    "all_df = pd.DataFrame(all_recs)\n",
    "print(f\"{len(all_df)} records total\")\n",
    "\n",
    "print('STRAGGLERS Follow:')\n",
    "straggler_df = all_df[(all_df.note.str.startswith('straggler'))]\n",
    "display(straggler_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the results DataFrame to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.to_csv('/tmp/all_publishable_datasets.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (hubmapEnv)",
   "language": "python",
   "name": "hubmapenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
